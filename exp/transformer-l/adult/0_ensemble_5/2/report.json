{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            10,
            11,
            12,
            13,
            14
        ]
    },
    "single_model_program": "bin/train1___6c0cf428316c490f83dc16aeb8290324.py",
    "data": "data/adult",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.9003059291992425,
                "recall": 0.9375474083438685,
                "f1-score": 0.918549346016647,
                "support": 19775
            },
            "1": {
                "precision": 0.773602199816682,
                "recall": 0.6727243743025666,
                "f1-score": 0.7196452933151433,
                "support": 6273
            },
            "accuracy": 0.8737714987714987,
            "macro avg": {
                "precision": 0.8369540645079623,
                "recall": 0.8051358913232176,
                "f1-score": 0.819097319665895,
                "support": 26048
            },
            "weighted avg": {
                "precision": 0.8697925502673934,
                "recall": 0.8737714987714987,
                "f1-score": 0.8706483508309693,
                "support": 26048
            },
            "roc_auc": 0.9313855278063452,
            "score": 0.8737714987714987
        },
        "val": {
            "0": {
                "precision": 0.8937949815211048,
                "recall": 0.9292214357937311,
                "f1-score": 0.9111639896886773,
                "support": 4945
            },
            "1": {
                "precision": 0.7448979591836735,
                "recall": 0.6517857142857143,
                "f1-score": 0.6952380952380953,
                "support": 1568
            },
            "accuracy": 0.8624289881774911,
            "macro avg": {
                "precision": 0.8193464703523892,
                "recall": 0.7905035750397227,
                "f1-score": 0.8032010424633863,
                "support": 6513
            },
            "weighted avg": {
                "precision": 0.8579481319855463,
                "recall": 0.8624289881774911,
                "f1-score": 0.8591799880767453,
                "support": 6513
            },
            "roc_auc": 0.9189011524731228,
            "score": 0.8624289881774911
        },
        "test": {
            "0": {
                "precision": 0.890693557077977,
                "recall": 0.930518697225573,
                "f1-score": 0.9101706914182334,
                "support": 12435
            },
            "1": {
                "precision": 0.737386018237082,
                "recall": 0.6307852314092564,
                "f1-score": 0.6799327354260088,
                "support": 3846
            },
            "accuracy": 0.8597137767950371,
            "macro avg": {
                "precision": 0.8140397876575296,
                "recall": 0.7806519643174147,
                "f1-score": 0.7950517134221211,
                "support": 16281
            },
            "weighted avg": {
                "precision": 0.8544782880906862,
                "recall": 0.8597137767950371,
                "f1-score": 0.8557824364740595,
                "support": 16281
            },
            "roc_auc": 0.9139657785748503,
            "score": 0.8597137767950371
        }
    }
}
