{
    "program": "bin/train1___38184151af07455eadb9e35eb2fa1e7b.py",
    "environment": {
        "CUDA_VISIBLE_DEVICES": "1",
        "gpus": {
            "driver": "460.106.00",
            "devices": [
                {
                    "name": "GeForce RTX 2080 Ti",
                    "memory_total": 11554717696,
                    "memory_free": 11069358080,
                    "memory_used": 485359616,
                    "utilization": 0
                },
                {
                    "name": "GeForce RTX 2080 Ti",
                    "memory_total": 11552096256,
                    "memory_free": 11548033024,
                    "memory_used": 4063232,
                    "utilization": 0
                }
            ]
        },
        "torch.version.cuda": "11.1",
        "torch.backends.cudnn.version()": 8005,
        "torch.cuda.nccl.version()": [
            2,
            10,
            3
        ]
    },
    "config": {
        "seed": 12,
        "data": {
            "path": "data/eye",
            "T": {
                "seed": 0,
                "normalization": "standard",
                "num_nan_policy": null,
                "cat_nan_policy": null,
                "cat_min_frequency": null,
                "cat_encoding": null,
                "y_policy": "default"
            },
            "T_cache": true
        },
        "model": {
            "d_num_embedding": 30,
            "num_embedding_arch": [
                "linear",
                "relu"
            ],
            "d_cat_embedding": null,
            "mlp": null,
            "resnet": {
                "n_blocks": 7,
                "d_main": 456,
                "dropout_first": 0.3805320059265692,
                "dropout_second": 0.0,
                "d_hidden": 1629
            },
            "transformer": null,
            "transformer_default": false,
            "transformer_baseline": true,
            "memory_efficient": false
        },
        "training": {
            "batch_size": 128,
            "lr": 0.003835043685426313,
            "weight_decay": 0.0,
            "optimizer": "AdamW",
            "patience": 16,
            "n_epochs": Infinity,
            "eval_batch_size": 8192
        },
        "bins": {
            "count": 79,
            "tree": {
                "min_samples_leaf": 125,
                "min_impurity_decrease": 0.0002870671213297444
            },
            "subsample": null
        }
    },
    "prediction_type": "logits",
    "epoch_size": 55,
    "n_parameters": 10798434,
    "best_epoch": 96,
    "metrics": {
        "train": {
            "0": {
                "precision": 0.9991776315789473,
                "recall": 0.9983566146261298,
                "f1-score": 0.9987669543773119,
                "support": 2434
            },
            "1": {
                "precision": 0.9985347985347985,
                "recall": 0.9996332966629996,
                "f1-score": 0.9990837456477917,
                "support": 2727
            },
            "2": {
                "precision": 1.0,
                "recall": 0.9994556341861731,
                "f1-score": 0.9997277429893819,
                "support": 1837
            },
            "accuracy": 0.9991426121749071,
            "macro avg": {
                "precision": 0.999237476704582,
                "recall": 0.9991485151584342,
                "f1-score": 0.9991928143381618,
                "support": 6998
            },
            "weighted avg": {
                "precision": 0.9991430052682987,
                "recall": 0.9991426121749071,
                "f1-score": 0.999142612919034,
                "support": 6998
            },
            "score": 0.9991426121749071
        },
        "val": {
            "0": {
                "precision": 0.7210365853658537,
                "recall": 0.7766830870279147,
                "f1-score": 0.7478260869565218,
                "support": 609
            },
            "1": {
                "precision": 0.7847533632286996,
                "recall": 0.7697947214076246,
                "f1-score": 0.7772020725388602,
                "support": 682
            },
            "2": {
                "precision": 0.9011764705882352,
                "recall": 0.8344226579520697,
                "f1-score": 0.8665158371040724,
                "support": 459
            },
            "accuracy": 0.7891428571428571,
            "macro avg": {
                "precision": 0.8023221397275963,
                "recall": 0.7936334887958697,
                "f1-score": 0.7971813321998181,
                "support": 1750
            },
            "weighted avg": {
                "precision": 0.7931160424055875,
                "recall": 0.7891428571428571,
                "f1-score": 0.7904049540907393,
                "support": 1750
            },
            "score": 0.7891428571428571
        },
        "test": {
            "0": {
                "precision": 0.7248743718592965,
                "recall": 0.7582128777923784,
                "f1-score": 0.7411689145793192,
                "support": 761
            },
            "1": {
                "precision": 0.7575030012004802,
                "recall": 0.7397420867526378,
                "f1-score": 0.7485172004744958,
                "support": 853
            },
            "2": {
                "precision": 0.8586762075134168,
                "recall": 0.8362369337979094,
                "f1-score": 0.8473080317740511,
                "support": 574
            },
            "accuracy": 0.7714808043875686,
            "macro avg": {
                "precision": 0.7803511935243979,
                "recall": 0.7780639661143086,
                "f1-score": 0.778998048942622,
                "support": 2188
            },
            "weighted avg": {
                "precision": 0.7726963437484622,
                "recall": 0.7714808043875686,
                "f1-score": 0.7718782112604717,
                "support": 2188
            },
            "score": 0.7714808043875686
        }
    },
    "time": "0:01:09"
}
