{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            5,
            6,
            7,
            8,
            9
        ]
    },
    "single_model_program": "bin/catboost____6cfbdc3bccc24d6f82837b38f0759e5e.py",
    "data": "data/adult",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.9054430258231737,
                "recall": 0.9539317319848293,
                "f1-score": 0.9290551355610825,
                "support": 19775
            },
            "1": {
                "precision": 0.8252780974299961,
                "recall": 0.6859556830862427,
                "f1-score": 0.7491947418821276,
                "support": 6273
            },
            "accuracy": 0.8893964987714987,
            "macro avg": {
                "precision": 0.8653605616265849,
                "recall": 0.819943707535536,
                "f1-score": 0.8391249387216051,
                "support": 26048
            },
            "weighted avg": {
                "precision": 0.8861373364877008,
                "recall": 0.8893964987714987,
                "f1-score": 0.8857403225409626,
                "support": 26048
            },
            "roc_auc": 0.9422010127887402,
            "score": 0.8893964987714987
        },
        "val": {
            "0": {
                "precision": 0.8941605839416058,
                "recall": 0.9413549039433772,
                "f1-score": 0.9171510196039798,
                "support": 4945
            },
            "1": {
                "precision": 0.7781178270849273,
                "recall": 0.6485969387755102,
                "f1-score": 0.7074782608695652,
                "support": 1568
            },
            "accuracy": 0.8708736373407032,
            "macro avg": {
                "precision": 0.8361392055132666,
                "recall": 0.7949759213594436,
                "f1-score": 0.8123146402367725,
                "support": 6513
            },
            "weighted avg": {
                "precision": 0.8662233748595741,
                "recall": 0.8708736373407032,
                "f1-score": 0.8666724558552369,
                "support": 6513
            },
            "roc_auc": 0.9293658818431316,
            "score": 0.8708736373407032
        },
        "test": {
            "0": {
                "precision": 0.8966493268053856,
                "recall": 0.9425814234016888,
                "f1-score": 0.9190418316540557,
                "support": 12435
            },
            "1": {
                "precision": 0.7775007790588968,
                "recall": 0.6487259490379615,
                "f1-score": 0.7072997873848335,
                "support": 3846
            },
            "accuracy": 0.8731650390025183,
            "macro avg": {
                "precision": 0.8370750529321411,
                "recall": 0.7956536862198251,
                "f1-score": 0.8131708095194445,
                "support": 16281
            },
            "weighted avg": {
                "precision": 0.8685033090771751,
                "recall": 0.8731650390025183,
                "f1-score": 0.8690227970579358,
                "support": 16281
            },
            "roc_auc": 0.9255627442628866,
            "score": 0.8731650390025183
        }
    }
}
