{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            5,
            6,
            7,
            8,
            9
        ]
    },
    "single_model_program": "bin/train3___29cb06878a7b47c7965aedacffadb92c.py",
    "data": "data/higgs-small",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.7549455551276216,
                "recall": 0.7288891893719154,
                "f1-score": 0.7416885953597167,
                "support": 29582
            },
            "1": {
                "precision": 0.765428487861948,
                "recall": 0.788989719316229,
                "f1-score": 0.7770305378642794,
                "support": 33169
            },
            "accuracy": 0.7606572006820609,
            "macro avg": {
                "precision": 0.7601870214947848,
                "recall": 0.7589394543440722,
                "f1-score": 0.7593595666119981,
                "support": 62751
            },
            "weighted avg": {
                "precision": 0.7604866364787534,
                "recall": 0.7606572006820609,
                "f1-score": 0.760369682369228,
                "support": 62751
            },
            "roc_auc": 0.8467722941235712,
            "score": 0.7606572006820609
        },
        "val": {
            "0": {
                "precision": 0.7390423839143643,
                "recall": 0.6907787993510005,
                "f1-score": 0.7140960234817249,
                "support": 7396
            },
            "1": {
                "precision": 0.7393732193732194,
                "recall": 0.7824409068982151,
                "f1-score": 0.760297650436515,
                "support": 8292
            },
            "accuracy": 0.7392274349821519,
            "macro avg": {
                "precision": 0.7392078016437919,
                "recall": 0.7366098531246078,
                "f1-score": 0.73719683695912,
                "support": 15688
            },
            "weighted avg": {
                "precision": 0.7392172492652584,
                "recall": 0.7392274349821519,
                "f1-score": 0.7385162102938819,
                "support": 15688
            },
            "roc_auc": 0.8195919744626696,
            "score": 0.7392274349821519
        },
        "test": {
            "0": {
                "precision": 0.7271490133817192,
                "recall": 0.6935640886965928,
                "f1-score": 0.7099595858938161,
                "support": 9245
            },
            "1": {
                "precision": 0.7374907338769459,
                "recall": 0.7678726483357453,
                "f1-score": 0.7523751004395708,
                "support": 10365
            },
            "accuracy": 0.7328403875573687,
            "macro avg": {
                "precision": 0.7323198736293326,
                "recall": 0.7307183685161691,
                "f1-score": 0.7311673431666934,
                "support": 19610
            },
            "weighted avg": {
                "precision": 0.7326152006807006,
                "recall": 0.7328403875573687,
                "f1-score": 0.7323785970241958,
                "support": 19610
            },
            "roc_auc": 0.8123940999385073,
            "score": 0.7328403875573687
        }
    }
}
