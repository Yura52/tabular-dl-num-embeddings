{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            10,
            11,
            12,
            13,
            14
        ]
    },
    "single_model_program": "bin/train0___0d72dc68c1b0427ba77d1efffa4f3d3b.py",
    "data": "data/adult",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.9056752237631553,
                "recall": 0.9312768647281922,
                "f1-score": 0.9182976389339051,
                "support": 19775
            },
            "1": {
                "precision": 0.7621631081554078,
                "recall": 0.6942451777458951,
                "f1-score": 0.7266205055476767,
                "support": 6273
            },
            "accuracy": 0.8741937960687961,
            "macro avg": {
                "precision": 0.8339191659592815,
                "recall": 0.8127610212370436,
                "f1-score": 0.822459072240791,
                "support": 26048
            },
            "weighted avg": {
                "precision": 0.8711139714133626,
                "recall": 0.8741937960687961,
                "f1-score": 0.8721370639288448,
                "support": 26048
            },
            "roc_auc": 0.932089780152654,
            "score": 0.8741937960687961
        },
        "val": {
            "0": {
                "precision": 0.8965924758715778,
                "recall": 0.920525783619818,
                "f1-score": 0.9084015166633407,
                "support": 4945
            },
            "1": {
                "precision": 0.7263231197771588,
                "recall": 0.6651785714285714,
                "f1-score": 0.6944074567243674,
                "support": 1568
            },
            "accuracy": 0.8590511285122063,
            "macro avg": {
                "precision": 0.8114577978243682,
                "recall": 0.7928521775241947,
                "f1-score": 0.8014044866938541,
                "support": 6513
            },
            "weighted avg": {
                "precision": 0.8556002525710943,
                "recall": 0.8590511285122063,
                "f1-score": 0.8568826028011711,
                "support": 6513
            },
            "roc_auc": 0.9153008346918142,
            "score": 0.8590511285122063
        },
        "test": {
            "0": {
                "precision": 0.8947779862414009,
                "recall": 0.9204664254121432,
                "f1-score": 0.9074404407975583,
                "support": 12435
            },
            "1": {
                "precision": 0.7165376898824878,
                "recall": 0.6500260010400416,
                "f1-score": 0.6816632583503749,
                "support": 3846
            },
            "accuracy": 0.8565812910754868,
            "macro avg": {
                "precision": 0.8056578380619444,
                "recall": 0.7852462132260924,
                "f1-score": 0.7945518495739665,
                "support": 16281
            },
            "weighted avg": {
                "precision": 0.8526729447945377,
                "recall": 0.8565812910754868,
                "f1-score": 0.8541059377761303,
                "support": 16281
            },
            "roc_auc": 0.9116668454434195,
            "score": 0.8565812910754868
        }
    }
}
