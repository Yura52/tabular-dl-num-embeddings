{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            10,
            11,
            12,
            13,
            14
        ]
    },
    "single_model_program": "bin/train1___669388609c9c4ac18439b3fde2074639.py",
    "data": "data/adult",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.9104433930052193,
                "recall": 0.9438685208596713,
                "f1-score": 0.9268547025523887,
                "support": 19775
            },
            "1": {
                "precision": 0.7998918334234721,
                "recall": 0.7073170731707317,
                "f1-score": 0.750761421319797,
                "support": 6273
            },
            "accuracy": 0.8869011056511057,
            "macro avg": {
                "precision": 0.8551676132143458,
                "recall": 0.8255927970152015,
                "f1-score": 0.8388080619360928,
                "support": 26048
            },
            "weighted avg": {
                "precision": 0.883819854412763,
                "recall": 0.8869011056511057,
                "f1-score": 0.8844471029987935,
                "support": 26048
            },
            "roc_auc": 0.945442134260712,
            "score": 0.8869011056511057
        },
        "val": {
            "0": {
                "precision": 0.8978484202364799,
                "recall": 0.9367037411526795,
                "f1-score": 0.9168646080760096,
                "support": 4945
            },
            "1": {
                "precision": 0.7688330871491876,
                "recall": 0.6639030612244898,
                "f1-score": 0.7125256673511293,
                "support": 1568
            },
            "accuracy": 0.8710271764163979,
            "macro avg": {
                "precision": 0.8333407536928338,
                "recall": 0.8003034011885847,
                "f1-score": 0.8146951377135694,
                "support": 6513
            },
            "weighted avg": {
                "precision": 0.8667880728879656,
                "recall": 0.8710271764163979,
                "f1-score": 0.8676701571230522,
                "support": 6513
            },
            "roc_auc": 0.9275067838055342,
            "score": 0.8710271764163979
        },
        "test": {
            "0": {
                "precision": 0.8990555813593436,
                "recall": 0.9339766787293928,
                "f1-score": 0.916183489133436,
                "support": 12435
            },
            "1": {
                "precision": 0.7558727326791556,
                "recall": 0.6609464378575143,
                "f1-score": 0.7052295741434318,
                "support": 3846
            },
            "accuracy": 0.8694797616854002,
            "macro avg": {
                "precision": 0.8274641570192496,
                "recall": 0.7974615582934536,
                "f1-score": 0.8107065316384339,
                "support": 16281
            },
            "weighted avg": {
                "precision": 0.8652320302246466,
                "recall": 0.8694797616854002,
                "f1-score": 0.8663506313819738,
                "support": 16281
            },
            "roc_auc": 0.9218542557544682,
            "score": 0.8694797616854002
        }
    }
}
