{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            5,
            6,
            7,
            8,
            9
        ]
    },
    "single_model_program": "bin/train3___95948b29b25d45a7933e7f47c8b6e29c.py",
    "data": "data/higgs-small",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.74403275741536,
                "recall": 0.7555270096680414,
                "f1-score": 0.749735831334597,
                "support": 29582
            },
            "1": {
                "precision": 0.7789190511127415,
                "recall": 0.768187162712171,
                "f1-score": 0.7735158847012037,
                "support": 33169
            },
            "accuracy": 0.7622189287820115,
            "macro avg": {
                "precision": 0.7614759042640508,
                "recall": 0.7618570861901062,
                "f1-score": 0.7616258580179003,
                "support": 62751
            },
            "weighted avg": {
                "precision": 0.7624729970234689,
                "recall": 0.7622189287820115,
                "f1-score": 0.7623055209031613,
                "support": 62751
            },
            "roc_auc": 0.8479277382828967,
            "score": 0.7622189287820115
        },
        "val": {
            "0": {
                "precision": 0.7204477204477204,
                "recall": 0.7136289886425095,
                "f1-score": 0.7170221437304715,
                "support": 7396
            },
            "1": {
                "precision": 0.7467113130829945,
                "recall": 0.7530149541726966,
                "f1-score": 0.749849885913294,
                "support": 8292
            },
            "accuracy": 0.7344467108618052,
            "macro avg": {
                "precision": 0.7335795167653575,
                "recall": 0.733321971407603,
                "f1-score": 0.7334360148218828,
                "support": 15688
            },
            "weighted avg": {
                "precision": 0.7343295224703933,
                "recall": 0.7344467108618052,
                "f1-score": 0.7343734720183326,
                "support": 15688
            },
            "roc_auc": 0.8156642832712014,
            "score": 0.7344467108618052
        },
        "test": {
            "0": {
                "precision": 0.7084713171669693,
                "recall": 0.7173607355327204,
                "f1-score": 0.7128883155971193,
                "support": 9245
            },
            "1": {
                "precision": 0.7450482973948678,
                "recall": 0.7367100820067535,
                "f1-score": 0.7408557291161346,
                "support": 10365
            },
            "accuracy": 0.7275879653238144,
            "macro avg": {
                "precision": 0.7267598072809185,
                "recall": 0.7270354087697369,
                "f1-score": 0.726872022356627,
                "support": 19610
            },
            "weighted avg": {
                "precision": 0.7278043309386251,
                "recall": 0.7275879653238144,
                "f1-score": 0.7276706838339675,
                "support": 19610
            },
            "roc_auc": 0.8089622035300499,
            "score": 0.7275879653238144
        }
    }
}
