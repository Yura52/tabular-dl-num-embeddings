{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            10,
            11,
            12,
            13,
            14
        ]
    },
    "single_model_program": "bin/train0___1971e84ec90e4ca4927a4d043684017f.py",
    "data": "data/adult",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.8998248516103922,
                "recall": 0.9352718078381795,
                "f1-score": 0.9172059808078553,
                "support": 19775
            },
            "1": {
                "precision": 0.7670185657080452,
                "recall": 0.6717678941495298,
                "f1-score": 0.7162403331350387,
                "support": 6273
            },
            "accuracy": 0.8718135749385749,
            "macro avg": {
                "precision": 0.8334217086592186,
                "recall": 0.8035198509938546,
                "f1-score": 0.816723156971447,
                "support": 26048
            },
            "weighted avg": {
                "precision": 0.8678418267537651,
                "recall": 0.8718135749385749,
                "f1-score": 0.8688085027730127,
                "support": 26048
            },
            "roc_auc": 0.928721256975342,
            "score": 0.8718135749385749
        },
        "val": {
            "0": {
                "precision": 0.8917234664070107,
                "recall": 0.9259858442871588,
                "f1-score": 0.908531746031746,
                "support": 4945
            },
            "1": {
                "precision": 0.7343976777939042,
                "recall": 0.6454081632653061,
                "f1-score": 0.6870332654446707,
                "support": 1568
            },
            "accuracy": 0.8584369722094273,
            "macro avg": {
                "precision": 0.8130605721004575,
                "recall": 0.7856970037762325,
                "f1-score": 0.7977825057382084,
                "support": 6513
            },
            "weighted avg": {
                "precision": 0.8538473975377721,
                "recall": 0.8584369722094273,
                "f1-score": 0.8552061483715995,
                "support": 6513
            },
            "roc_auc": 0.9166952549472772,
            "score": 0.8584369722094273
        },
        "test": {
            "0": {
                "precision": 0.8913883735312307,
                "recall": 0.9273019702452754,
                "f1-score": 0.9089905797958299,
                "support": 12435
            },
            "1": {
                "precision": 0.729745889387145,
                "recall": 0.6346853874154966,
                "f1-score": 0.6789041857877903,
                "support": 3846
            },
            "accuracy": 0.8581782445795713,
            "macro avg": {
                "precision": 0.8105671314591878,
                "recall": 0.780993678830386,
                "f1-score": 0.7939473827918101,
                "support": 16281
            },
            "weighted avg": {
                "precision": 0.8532041714540761,
                "recall": 0.8581782445795713,
                "f1-score": 0.8546381277747672,
                "support": 16281
            },
            "roc_auc": 0.913481189026411,
            "score": 0.8581782445795713
        }
    }
}
