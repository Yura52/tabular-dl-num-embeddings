{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            5,
            6,
            7,
            8,
            9
        ]
    },
    "single_model_program": "bin/train3___c2d0df569bd6462786fe68f2c64a4228.py",
    "data": "data/higgs-small",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.750300922830012,
                "recall": 0.758569400311,
                "f1-score": 0.7544125063035804,
                "support": 29582
            },
            "1": {
                "precision": 0.7825411807691136,
                "recall": 0.7748500105520215,
                "f1-score": 0.778676604253772,
                "support": 33169
            },
            "accuracy": 0.7671750250992017,
            "macro avg": {
                "precision": 0.7664210517995629,
                "recall": 0.7667097054315107,
                "f1-score": 0.7665445552786763,
                "support": 62751
            },
            "weighted avg": {
                "precision": 0.7673425176345898,
                "recall": 0.7671750250992017,
                "f1-score": 0.7672380527476196,
                "support": 62751
            },
            "roc_auc": 0.8536616674284405,
            "score": 0.7671750250992017
        },
        "val": {
            "0": {
                "precision": 0.7276366648260629,
                "recall": 0.7126825310978907,
                "f1-score": 0.7200819672131147,
                "support": 7396
            },
            "1": {
                "precision": 0.7483420180009475,
                "recall": 0.7620598166907863,
                "f1-score": 0.7551386233269598,
                "support": 8292
            },
            "accuracy": 0.7387812340642529,
            "macro avg": {
                "precision": 0.7379893414135053,
                "recall": 0.7373711738943385,
                "f1-score": 0.7376102952700372,
                "support": 15688
            },
            "weighted avg": {
                "precision": 0.7385806212593968,
                "recall": 0.7387812340642529,
                "f1-score": 0.7386114032467712,
                "support": 15688
            },
            "roc_auc": 0.8189339594915389,
            "score": 0.7387812340642529
        },
        "test": {
            "0": {
                "precision": 0.7164660361134996,
                "recall": 0.7210383991346674,
                "f1-score": 0.7187449458191816,
                "support": 9245
            },
            "1": {
                "precision": 0.7497574228604696,
                "recall": 0.7454896285576459,
                "f1-score": 0.7476174350539404,
                "support": 10365
            },
            "accuracy": 0.7339622641509433,
            "macro avg": {
                "precision": 0.7331117294869847,
                "recall": 0.7332640138461566,
                "f1-score": 0.733181190436561,
                "support": 19610
            },
            "weighted avg": {
                "precision": 0.734062426915761,
                "recall": 0.7339622641509433,
                "f1-score": 0.7340056980332701,
                "support": 19610
            },
            "roc_auc": 0.8124504634387317,
            "score": 0.7339622641509433
        }
    }
}
