{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            5,
            6,
            7,
            8,
            9
        ]
    },
    "single_model_program": "bin/train3___b38dcfb99e324992b03090fdb0a2c3ff.py",
    "data": "data/higgs-small",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.7615163147792706,
                "recall": 0.7510648367250355,
                "f1-score": 0.7562544674767692,
                "support": 29582
            },
            "1": {
                "precision": 0.7806701414743112,
                "recall": 0.7902258132593687,
                "f1-score": 0.7854189140596909,
                "support": 33169
            },
            "accuracy": 0.771764593392934,
            "macro avg": {
                "precision": 0.7710932281267909,
                "recall": 0.7706453249922021,
                "f1-score": 0.77083669076823,
                "support": 62751
            },
            "weighted avg": {
                "precision": 0.7716406678198245,
                "recall": 0.771764593392934,
                "f1-score": 0.7716702461688845,
                "support": 62751
            },
            "roc_auc": 0.8549303376307082,
            "score": 0.771764593392934
        },
        "val": {
            "0": {
                "precision": 0.7221909321317939,
                "recall": 0.6934829637641968,
                "f1-score": 0.7075458683956408,
                "support": 7396
            },
            "1": {
                "precision": 0.7359655252737014,
                "recall": 0.7620598166907863,
                "f1-score": 0.748785401113876,
                "support": 8292
            },
            "accuracy": 0.7297297297297297,
            "macro avg": {
                "precision": 0.7290782287027476,
                "recall": 0.7277713902274916,
                "f1-score": 0.7281656347547585,
                "support": 15688
            },
            "weighted avg": {
                "precision": 0.7294715878133784,
                "recall": 0.7297297297297297,
                "f1-score": 0.729343306265325,
                "support": 15688
            },
            "roc_auc": 0.8109485867642827,
            "score": 0.7297297297297297
        },
        "test": {
            "0": {
                "precision": 0.7119055500386186,
                "recall": 0.6978907517577069,
                "f1-score": 0.7048284902774744,
                "support": 9245
            },
            "1": {
                "precision": 0.7351853607660946,
                "recall": 0.7480945489628558,
                "f1-score": 0.741583779648049,
                "support": 10365
            },
            "accuracy": 0.7244263131055584,
            "macro avg": {
                "precision": 0.7235454554023566,
                "recall": 0.7229926503602813,
                "f1-score": 0.7232061349627616,
                "support": 19610
            },
            "weighted avg": {
                "precision": 0.7242102536689241,
                "recall": 0.7244263131055584,
                "f1-score": 0.7242557505694686,
                "support": 19610
            },
            "roc_auc": 0.8037916533284702,
            "score": 0.7244263131055584
        }
    }
}
