{
    "program": "bin/train3___5c6d4fb0d9604632b2c5fb7900de236c.py",
    "environment": {
        "CUDA_VISIBLE_DEVICES": "2",
        "gpus": {
            "driver": "470.63.01",
            "devices": [
                {
                    "name": "NVIDIA A100-SXM-80GB",
                    "memory_total": 85198045184,
                    "memory_free": 50069700608,
                    "memory_used": 35128344576,
                    "utilization": 20
                },
                {
                    "name": "NVIDIA A100-SXM-80GB",
                    "memory_total": 85198045184,
                    "memory_free": 57136054272,
                    "memory_used": 28061990912,
                    "utilization": 20
                },
                {
                    "name": "NVIDIA A100-SXM-80GB",
                    "memory_total": 85198045184,
                    "memory_free": 39243153408,
                    "memory_used": 45954891776,
                    "utilization": 0
                },
                {
                    "name": "NVIDIA A100-SXM-80GB",
                    "memory_total": 85198045184,
                    "memory_free": 30512709632,
                    "memory_used": 54685335552,
                    "utilization": 0
                },
                {
                    "name": "NVIDIA A100-SXM-80GB",
                    "memory_total": 85198045184,
                    "memory_free": 9567404032,
                    "memory_used": 75630641152,
                    "utilization": 100
                },
                {
                    "name": "NVIDIA A100-SXM-80GB",
                    "memory_total": 85198045184,
                    "memory_free": 10820452352,
                    "memory_used": 74377592832,
                    "utilization": 86
                }
            ]
        },
        "torch.version.cuda": "11.1",
        "torch.backends.cudnn.version()": 8005,
        "torch.cuda.nccl.version()": [
            2,
            10,
            3
        ]
    },
    "config": {
        "seed": 2,
        "data": {
            "path": "data/fb-comments",
            "T": {
                "seed": 0,
                "normalization": "quantile",
                "num_nan_policy": null,
                "cat_nan_policy": null,
                "cat_min_frequency": null,
                "cat_encoding": null,
                "y_policy": "default"
            },
            "T_cache": true
        },
        "model": {
            "d_num_embedding": 13,
            "num_embedding_arch": [
                "positional",
                "linear",
                "relu"
            ],
            "d_cat_embedding": null,
            "mlp": {
                "d_layers": [
                    613
                ],
                "dropout": 0.41946923260241553
            },
            "resnet": null,
            "transformer": null,
            "transformer_default": false,
            "transformer_baseline": true,
            "periodic_sigma": null,
            "positional_encoding": {
                "n": 27,
                "sigma": 0.00803212022026809,
                "trainable": true,
                "initialization": "log-linear"
            },
            "fourier_features": null,
            "memory_efficient": false
        },
        "training": {
            "batch_size": 512,
            "lr": 9.974723336793861e-05,
            "weight_decay": 0.0,
            "optimizer": "AdamW",
            "patience": 16,
            "n_epochs": Infinity,
            "eval_batch_size": 8192
        },
        "bins": null
    },
    "prediction_type": null,
    "epoch_size": 308,
    "n_parameters": 486430,
    "best_epoch": 222,
    "metrics": {
        "train": {
            "rmse": 3.245385030226711,
            "score": -3.245385030226711
        },
        "val": {
            "rmse": 5.11395598166116,
            "score": -5.11395598166116
        },
        "test": {
            "rmse": 5.439521371883393,
            "score": -5.439521371883393
        }
    },
    "time": "0:03:21"
}
