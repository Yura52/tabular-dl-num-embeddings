{
    "program": "bin/ensemble.py",
    "config": {
        "seeds": [
            0,
            1,
            2,
            3,
            4
        ]
    },
    "single_model_program": "bin/train1___6b57fd44df724ec1aed7f95ab1312e8f.py",
    "data": "data/higgs-small",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.762310670379678,
                "recall": 0.7588060306943412,
                "f1-score": 0.7605543132072914,
                "support": 29582
            },
            "1": {
                "precision": 0.7857679027173097,
                "recall": 0.788989719316229,
                "f1-score": 0.7873755152390407,
                "support": 33169
            },
            "accuracy": 0.774760561584676,
            "macro avg": {
                "precision": 0.7740392865484939,
                "recall": 0.7738978750052851,
                "f1-score": 0.773964914223166,
                "support": 62751
            },
            "weighted avg": {
                "precision": 0.7747097228156058,
                "recall": 0.774760561584676,
                "f1-score": 0.7747314968408764,
                "support": 62751
            },
            "roc_auc": 0.8599719244500905,
            "score": 0.774760561584676
        },
        "val": {
            "0": {
                "precision": 0.7263274643005684,
                "recall": 0.7083558680367766,
                "f1-score": 0.7172291053460196,
                "support": 7396
            },
            "1": {
                "precision": 0.7454867256637168,
                "recall": 0.7619392185238785,
                "f1-score": 0.7536231884057971,
                "support": 8292
            },
            "accuracy": 0.7366777154513003,
            "macro avg": {
                "precision": 0.7359070949821426,
                "recall": 0.7351475432803276,
                "f1-score": 0.7354261468759083,
                "support": 15688
            },
            "weighted avg": {
                "precision": 0.7364542233025587,
                "recall": 0.7366777154513003,
                "f1-score": 0.7364654475650197,
                "support": 15688
            },
            "roc_auc": 0.8193413777985102,
            "score": 0.7366777154513003
        },
        "test": {
            "0": {
                "precision": 0.719650655021834,
                "recall": 0.7130340724716063,
                "f1-score": 0.7163270850312415,
                "support": 9245
            },
            "1": {
                "precision": 0.7461244019138756,
                "recall": 0.7522431259044863,
                "f1-score": 0.749171270718232,
                "support": 10365
            },
            "accuracy": 0.7337582865884753,
            "macro avg": {
                "precision": 0.7328875284678549,
                "recall": 0.7326385991880463,
                "f1-score": 0.7327491778747368,
                "support": 19610
            },
            "weighted avg": {
                "precision": 0.7336435355183161,
                "recall": 0.7337582865884753,
                "f1-score": 0.7336871046460124,
                "support": 19610
            },
            "roc_auc": 0.814097647859614,
            "score": 0.7337582865884753
        }
    }
}
